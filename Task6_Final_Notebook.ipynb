{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-6 Complete Solution: Time Series (ARIMA) + Logistic Regression (Heart Disease)\n",
    "\n",
    "This notebook will:\n",
    "1) Create (or load if present) datasets.\n",
    "2) Perform full analysis for both projects.\n",
    "3) Generate all deliverables (plots, tables, saved models/files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Helper: Safe MAPE\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    eps = 1e-8\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Time Series Analysis with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create synthetic daily sales dataset (3 years)\n",
    "np.random.seed(42)\n",
    "start_date = datetime(2022, 1, 1)\n",
    "dates = pd.date_range(start_date, periods=3*365, freq='D')\n",
    "\n",
    "t = np.arange(len(dates))\n",
    "trend = 0.05 * t\n",
    "weekly = 10 * np.sin(2 * np.pi * t / 7)\n",
    "annual = 20 * np.sin(2 * np.pi * t / 365)\n",
    "noise = np.random.normal(0, 5, len(dates))\n",
    "\n",
    "base = 200 + trend + weekly + annual + noise\n",
    "sales = np.maximum(0, base).round(0)\n",
    "\n",
    "ts_df = pd.DataFrame({'Date': dates, 'Sales': sales})\n",
    "ts_df.to_csv('sales_timeseries.csv', index=False)\n",
    "\n",
    "print(\"Time series dataset created with\", len(ts_df), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Visualization (trend + moving average)\n",
    "ts_df['MA_30'] = ts_df['Sales'].rolling(window=30, min_periods=1).mean()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ts_df['Date'], ts_df['Sales'], label='Sales', linewidth=1)\n",
    "plt.plot(ts_df['Date'], ts_df['MA_30'], label='30-day MA', linewidth=2)\n",
    "plt.title('Sales Trend with 30-Day Moving Average')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('sales_trend.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Train/Test split: last 90 days as test\n",
    "train_df = ts_df.iloc[:-90].copy()\n",
    "test_df = ts_df.iloc[-90:].copy()\n",
    "\n",
    "y_train = train_df['Sales'].astype(float)\n",
    "\n",
    "print(\"Training data:\", len(train_df), \"records\")\n",
    "print(\"Test data:\", len(test_df), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best ARIMA order via a comprehensive grid search\n",
    "best_order = None\n",
    "best_aic = np.inf\n",
    "for p in range(3):  # 0, 1, 2\n",
    "    for d in range(2):  # 0, 1\n",
    "        for q in range(3):  # 0, 1, 2\n",
    "            try:\n",
    "                model = ARIMA(y_train, order=(p, d, q))\n",
    "                res = model.fit()\n",
    "                if res.aic < best_aic:\n",
    "                    best_aic = res.aic\n",
    "                    best_order = (p, d, q)\n",
    "                print(f\"ARIMA({p},{d},{q}): AIC = {res.aic:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f'Error fitting ARIMA({p},{d},{q}): {e}')\n",
    "\n",
    "print(f\"\\nBest ARIMA order: {best_order} with AIC: {best_aic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA with best order\n",
    "model = ARIMA(y_train, order=best_order)\n",
    "res = model.fit()\n",
    "\n",
    "steps = len(test_df)\n",
    "forecast_res = res.get_forecast(steps=steps)\n",
    "forecast_mean = forecast_res.predicted_mean\n",
    "forecast_ci = forecast_res.conf_int(alpha=0.05)\n",
    "\n",
    "# Evaluate\n",
    "rmse = math.sqrt(mean_squared_error(test_df['Sales'], forecast_mean))\n",
    "mape_val = mape(test_df['Sales'], forecast_mean)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save forecast results\n",
    "forecast_table = pd.DataFrame({\n",
    "    'Date': test_df['Date'].values,\n",
    "    'Actual_Sales': test_df['Sales'].values,\n",
    "    'Forecast_Sales': np.round(forecast_mean.values, 2),\n",
    "    'Lower_95_CI': np.round(forecast_ci.iloc[:, 0].values, 2),\n",
    "    'Upper_95_CI': np.round(forecast_ci.iloc[:, 1].values, 2),\n",
    "})\n",
    "forecast_table.to_csv('forecasted_sales.csv', index=False)\n",
    "print(\"Forecast results saved to forecasted_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_df['Date'], train_df['Sales'], label='Train', linewidth=1)\n",
    "plt.plot(test_df['Date'], test_df['Sales'], label='Actual', linewidth=1)\n",
    "plt.plot(test_df['Date'], forecast_mean, label=f'Forecast ARIMA{best_order}', linewidth=2)\n",
    "plt.fill_between(test_df['Date'], forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.2)\n",
    "plt.title(f'ARIMA Forecast vs Actual | RMSE={rmse:.2f}, MAPE={mape_val:.2f}%')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('sales_forecast.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Logistic Regression â€” Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'heart_disease.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    # Create synthetic dataset\n",
    "    np.random.seed(7)\n",
    "    N = 1000\n",
    "    age = np.random.randint(29, 78, size=N)\n",
    "    gender = np.random.choice(['Male', 'Female'], size=N, p=[0.6, 0.4])\n",
    "    cholesterol = np.random.normal(220, 35, size=N).clip(120, 380).astype(int)\n",
    "    systolic = np.random.normal(135, 18, size=N).clip(90, 220).astype(int)\n",
    "    diastolic = np.random.normal(85, 12, size=N).clip(50, 140).astype(int)\n",
    "\n",
    "    gender_flag = (gender == 'Male').astype(int)\n",
    "    risk = (\n",
    "        0.04*(age-50) + 0.02*(cholesterol-200) + 0.03*(systolic-120) + 0.02*(diastolic-80)\n",
    "        + 0.2*gender_flag + np.random.normal(0,1.0,size=N)\n",
    "    )\n",
    "    prob = 1/(1+np.exp(-0.02*(risk-np.mean(risk))))\n",
    "    heart_disease = (np.random.rand(N) < prob).astype(int)\n",
    "\n",
    "    heart_df = pd.DataFrame({\n",
    "        'Age': age,\n",
    "        'Gender': gender,\n",
    "        'Cholesterol': cholesterol,\n",
    "        'Blood Pressure': [f'{s}/{d}' for s,d in zip(systolic, diastolic)],\n",
    "        'Heart Disease': heart_disease\n",
    "    })\n",
    "    heart_df.to_csv(csv_path, index=False)\n",
    "    print(\"Heart disease dataset created with\", len(heart_df), \"records\")\n",
    "else:\n",
    "    heart_df = pd.read_csv(csv_path)\n",
    "    print(\"Loaded existing heart disease dataset with\", len(heart_df), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df = heart_df.drop_duplicates()\n",
    "df = df.ffill().bfill()\n",
    "df['Systolic'] = df['Blood Pressure'].apply(lambda x: int(str(x).split('/')[0]))\n",
    "df['Diastolic'] = df['Blood Pressure'].apply(lambda x: int(str(x).split('/')[1]))\n",
    "df['Gender'] = df['Gender'].map({'Male':1, 'Female':0})\n",
    "\n",
    "X = df[['Age','Gender','Cholesterol','Systolic','Diastolic']].astype(float)\n",
    "y = df['Heart Disease'].astype(int)\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.25,random_state=42,stratify=y)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "prec = precision_score(y_test,y_pred,zero_division=0)\n",
    "rec = recall_score(y_test,y_pred,zero_division=0)\n",
    "f1 = f1_score(y_test,y_pred,zero_division=0)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation report\n",
    "with open('heart_eval_report.txt','w') as f:\n",
    "    f.write('Logistic Regression â€” Heart Disease Evaluation\\n')\n",
    "    f.write(f'Accuracy:  {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1-score: {f1:.4f}\\n')\n",
    "    f.write('\\nClassification Report:\\n')\n",
    "    f.write(classification_report(y_test,y_pred,zero_division=0))\n",
    "\n",
    "print(\"Evaluation report saved to heart_eval_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.title('Confusion Matrix (Heart Disease)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.colorbar()\n",
    "for (i,j),v in np.ndenumerate(cm):\n",
    "    plt.text(j,i,str(v),ha='center',va='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('heart_confusion_matrix.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and scaler\n",
    "with open('heart_lr_model.pkl','wb') as f:\n",
    "    pickle.dump(lr,f)\n",
    "with open('heart_scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler,f)\n",
    "\n",
    "print(\"Model and scaler saved successfully\")\n",
    "print('âœ… All deliverables generated successfully in the working folder!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
